{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STOR 566, Homework 1\n",
        "### Instructor: Yao Li\n",
        "### Keywords: optimization\n",
        "### Due date: Sep 07, 11:55pm\n",
        "### **Submission Instruction**\n",
        "\n",
        "- Please download this script and use it to answer the questions in the homework. \n",
        "- For submission, please include your code, code output and answers in the script and submit the ipynb file on sakai.\n",
        "- Please don't modify existing cells. But you can add cells between the exercise statements.\n",
        "- To make markdown, please switch the cell type to markdown (from code) - you can hit 'm' when you are in command mode - and use the markdown language. For a brief tutorial see: https://daringfireball.net/projects/markdown/syntax\n"
      ],
      "metadata": {
        "id": "0D6giqCEScfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1 (10 points)\n",
        "\n",
        "Prove whether the following functions are convex or not.\n",
        "- (a) (5 points) $f(x_1,x_2)=(x_1x_2-1)^2$, where $x_1, x_2\\in \\mathbb{R}$.\n",
        "- (b) (5 points) $f(\\mathbf{w}_1, \\mathbf{w}_2)=\\|\\mathbf{w}_1-\\mathbf{w}_2\\|^2_2$,  where $\\mathbf{w}_1, \\mathbf{w}_2\\in \\mathbb{R}^2$.\n"
      ],
      "metadata": {
        "id": "rB6khqRukUzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of Problem 1 (a):"
      ],
      "metadata": {
        "id": "L_VY0XmUkteW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "6Tigk4RBkyFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of Problem 1 (b):"
      ],
      "metadata": {
        "id": "sIrHUR8zk1Tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "I0rrCHF0k32r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2 (10 points)\n",
        "\n",
        "Identify stationary points for $f(x) = 2x_1 +12x_2 +x^2_1 -3x^2_2$? Are they local minimum/maximum; global minimum/maximum or saddle points? Why?"
      ],
      "metadata": {
        "id": "HRihLtPqk6rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "PG1_3Bemk79P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3 (80 points)\n",
        "\n",
        "Given training data $\\{\\mathbf{x}_i, y_i\\}^n_{i=1}$, each $x_i\\in \\mathbb{R}^d$ and $y_i \\in \\{+1, -1\\}$, we try to solve the following logistic regression problem by gradient descent:\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{w}\\in R^d}\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\log(1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})+\\frac{1}{2}\\|\\mathbf{w}\\|^2_2  \\right\\} := f(\\mathbf{w}).\n",
        "\\end{align}\n",
        "Test the algorithm using the 'heart scale' dataset with $n = 270$ and $d = 13$: the matrix $\\mathbf{X}$ is stored in the file 'X$\\_$heart', and the vector $\\mathbf{y}$ is stored in the file 'y$\\_$heart'. ('X$\\_$heart' contains $n$ lines, each line stores a vector $\\mathbf{x}_i$ with $d$ real numbers. 'y$\\_$heart' contains the $\\mathbf{y}$ vector.)"
      ],
      "metadata": {
        "id": "kegz8m6glB3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Please implement data loading yourself"
      ],
      "metadata": {
        "id": "c1dXtcBGqZmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load data:"
      ],
      "metadata": {
        "id": "sJStVuRpqkzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) (5 points) Compute the gradient of $f(\\mathbf{w})$ w.r.t. $\\mathbf{w}$."
      ],
      "metadata": {
        "id": "vZWcD3ufnEcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "_KmlQojYnL1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) (30 points) Implement the gradient descent algorithm with a fixed step size $\\eta$. Find a small $\\eta_1$ such that the algorithm converges. Increase the step size to $\\eta_2$ so the algorithm cannot converge. Run 50 iterations and plot the iteration versus $\\log(f(\\mathbf{w}^k) -f(\\mathbf{w}^*))$ plot for $\\eta_1$ and $\\eta_2$. In practice it is impossible to get the exact optimal solution $\\mathbf{w}^*$, so use the minimum value you computed as $f(\\mathbf{w}^*)$ when you plot the figure. Report the $f(\\mathbf{w}^*)$ value you used for generating the plots."
      ],
      "metadata": {
        "id": "KYWHWEGinOyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementation of GD\n",
        "## You can insert more code chunks and text cells between (b) and (c) if you want to.\n",
        "## Your code:"
      ],
      "metadata": {
        "id": "L8pJfqcHm89-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot of a small step size $\\eta_1$:"
      ],
      "metadata": {
        "id": "IlxfxSzOnyjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot of a small step size $\\eta_2$:"
      ],
      "metadata": {
        "id": "NUYIuw_an2tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The $f(\\mathbf{x}^*)$ value used is: "
      ],
      "metadata": {
        "id": "iG6GtonCnuvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) (5 points) Write down the pseudo code of gradient descent with backtracking line search ($\\sigma=0.01$)."
      ],
      "metadata": {
        "id": "2CSl47RKppHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudo code:"
      ],
      "metadata": {
        "id": "yfYLYqLgp4i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) (20 points) Implement the gradient descent algorithm with backtracking line search ($\\sigma=0.01$). Plot the same iteration versus $\\log(f(\\mathbf{w}^k) -f(\\mathbf{w}^*))$ plot."
      ],
      "metadata": {
        "id": "peboWtMBp62-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementation of GD with line search\n",
        "## You can insert more code chunks and text cells between (d) and (e) if you want to.\n",
        "## Your code:"
      ],
      "metadata": {
        "id": "ff-v_MOZqDGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Plot:"
      ],
      "metadata": {
        "id": "mjG8nJHIqIAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) (20 points) Test your implementation (gradient descent with backtracking line search) on a larger dataset 'epsilonsubset'. Plot the same iteration vs error plot."
      ],
      "metadata": {
        "id": "VbHsQrUyqNCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementation of GD with line search\n",
        "## You can insert more code chunks and text cells between (d) and (e) if you want to.\n",
        "## Your code:"
      ],
      "metadata": {
        "id": "Uy7Tb8W8qUMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Plot:"
      ],
      "metadata": {
        "id": "9X_eNkjVqUtJ"
      }
    }
  ]
}